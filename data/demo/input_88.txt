88. Bioinformatics. 2020 Feb 15;36(4):1234-1240. doi: 10.1093/bioinformatics/btz682.

BioBERT: a pre-trained biomedical language representation model for biomedical 
text mining.

Lee J(1), Yoon W(1), Kim S(2), Kim D(1), Kim S(1), So CH(3), Kang J(1)(3).

Author information:
(1)Department of Computer Science and Engineering, Korea University, Seoul 
02841, Korea.
(2)Clova AI Research, Naver Corp, Seong-Nam 13561, Korea.
(3)Interdisciplinary Graduate Program in Bioinformatics, Korea University, Seoul 
02841, Korea.

MOTIVATION: Biomedical text mining is becoming increasingly important as the 
number of biomedical documents rapidly grows. With the progress in natural 
language processing (NLP), extracting valuable information from biomedical 
literature has gained popularity among researchers, and deep learning has 
boosted the development of effective biomedical text mining models. However, 
directly applying the advancements in NLP to biomedical text mining often yields 
unsatisfactory results due to a word distribution shift from general domain 
corpora to biomedical corpora. In this article, we investigate how the recently 
introduced pre-trained language model BERT can be adapted for biomedical 
corpora.
RESULTS: We introduce BioBERT (Bidirectional Encoder Representations from 
Transformers for Biomedical Text Mining), which is a domain-specific language 
representation model pre-trained on large-scale biomedical corpora. With almost 
the same architecture across tasks, BioBERT largely outperforms BERT and 
previous state-of-the-art models in a variety of biomedical text mining tasks 
when pre-trained on biomedical corpora. While BERT obtains performance 
comparable to that of previous state-of-the-art models, BioBERT significantly 
outperforms them on the following three representative biomedical text mining 
tasks: biomedical named entity recognition (0.62% F1 score improvement), 
biomedical relation extraction (2.80% F1 score improvement) and biomedical 
question answering (12.24% MRR improvement). Our analysis results show that 
pre-training BERT on biomedical corpora helps it to understand complex 
biomedical texts.
AVAILABILITY AND IMPLEMENTATION: We make the pre-trained weights of BioBERT 
freely available at https://github.com/naver/biobert-pretrained, and the source 
code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.

Â© The Author(s) 2019. Published by Oxford University Press.

DOI: 10.1093/bioinformatics/btz682
PMID: 31501885